# Context2Task - Roadmap de ImplementaÃ§Ã£o Detalhado

**Data**: 2025-10-19  
**VersÃ£o**: 1.0  
**PropÃ³sito**: Guia passo-a-passo para implementaÃ§Ã£o do MVP atÃ© V2

---

## ğŸ“Š VisÃ£o Geral

### Timeline Estimado
- **Fase 1 (MVP)**: 3 semanas (core functionality)
- **Fase 2 (Polish)**: 1 semana (UX + testing)
- **Fase 3 (Advanced)**: 2 semanas (features avanÃ§adas)
- **TOTAL**: ~6 semanas para V1 completo

### PriorizaÃ§Ã£o
- **P0 (CrÃ­tico)**: Sem isso, produto nÃ£o funciona
- **P1 (Alto)**: Impacto significativo na UX
- **P2 (MÃ©dio)**: Nice to have para V1
- **P3 (Baixo)**: V2+

---

## ğŸ—ï¸ FASE 1: MVP Core (Semanas 1-3)

### Sprint 1: FundaÃ§Ã£o (Dias 1-5)

#### ğŸ“¦ Task 1.1: Docker Setup (P0) - 1 dia
**Objetivo**: Ambiente local funcionando com 1 comando

**Deliverables**:
- `docker-compose.yml` completo
- Frontend Dockerfile (multi-stage build)
- Backend Dockerfile (Python 3.10+)
- `.env.example` com todas variÃ¡veis
- `scripts/start.sh` e `scripts/stop.sh`
- `README.md` com instruÃ§Ãµes

**ValidaÃ§Ã£o**:
```bash
$ docker-compose up
# âœ… Frontend acessÃ­vel em localhost:3000
# âœ… Backend acessÃ­vel em localhost:8000
# âœ… Health check passing
```

**Files**:
```
/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ nginx.conf
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ scripts/
    â”œâ”€â”€ start.sh
    â””â”€â”€ stop.sh
```

---

#### ğŸ¨ Task 1.2: Frontend Base (P0) - 2 dias
**Objetivo**: React app com shadcn/ui configurado

**Deliverables**:
- React 18 + TypeScript + Vite
- shadcn/ui instalado e configurado
- Tailwind CSS setup
- Estrutura de pastas
- Componentes base (Button, Input, Card, etc.)
- Routing (React Router)
- State management (Zustand)

**Estrutura**:
```
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ui/            # shadcn components
â”‚   â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ sidebar/
â”‚   â”‚   â””â”€â”€ preview/
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ Home.tsx
â”‚   â”‚   â”œâ”€â”€ Session.tsx
â”‚   â”‚   â””â”€â”€ Review.tsx
â”‚   â”œâ”€â”€ stores/
â”‚   â”‚   â””â”€â”€ session.ts     # Zustand store
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ client.ts      # API client
â”‚   â””â”€â”€ types/
â”‚       â””â”€â”€ index.ts       # TypeScript types
```

**ValidaÃ§Ã£o**:
- âœ… Componentes renderizam corretamente
- âœ… Tema dark/light funciona
- âœ… NavegaÃ§Ã£o entre pÃ¡ginas OK
- âœ… TypeScript sem erros

---

#### âš™ï¸ Task 1.3: Backend Base (P0) - 2 dias
**Objetivo**: FastAPI funcionando com estrutura base

**Deliverables**:
- FastAPI app
- Estrutura de pastas
- Session management (in-memory)
- Basic endpoints (health, sessions)
- CORS configurado
- Logging estruturado (structlog)
- Error handling

**Estrutura**:
```
backend/
â”œâ”€â”€ main.py                # FastAPI app
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ sessions.py
â”‚   â”œâ”€â”€ conversations.py
â”‚   â””â”€â”€ analysis.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ session.py
â”‚   â”œâ”€â”€ message.py
â”‚   â””â”€â”€ spec.py
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ llm.py            # OpenRouter client
â”‚   â”œâ”€â”€ mcp.py            # MCP client
â”‚   â””â”€â”€ spec_generator.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logging.py
â”‚   â”œâ”€â”€ sanitization.py
â”‚   â””â”€â”€ validation.py
â”œâ”€â”€ config.py
â””â”€â”€ tests/
```

**ValidaÃ§Ã£o**:
```bash
$ curl http://localhost:8000/health
{"status": "healthy"}

$ curl -X POST http://localhost:8000/api/sessions \
  -H "Content-Type: application/json" \
  -d '{"user_profile": "technical", "selected_repositories": ["/test"]}'
{"session_id": "sess_abc123", ...}
```

---

### Sprint 2: Integrations (Dias 6-10)

#### ğŸ”Œ Task 2.1: OpenRouter Integration (P0) - 1 dia
**Objetivo**: LLM funcionando com Gemini 2.5 Pro

**Deliverables**:
- OpenRouter client configurado
- Prompt templates implementados
- Streaming support
- Error handling + retry logic
- Token counting
- Cost tracking

**Code Example**:
```python
# backend/services/llm.py

from openai import AsyncOpenAI
from typing import List, Dict, AsyncGenerator

class LLMService:
    def __init__(self):
        self.client = AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=settings.openrouter_api_key
        )
        self.model = settings.default_llm_model
    
    async def chat(
        self,
        messages: List[Dict],
        stream: bool = False
    ) -> Dict | AsyncGenerator:
        """Chat with LLM"""
        
        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                stream=stream,
                extra_headers={
                    "HTTP-Referer": settings.app_url,
                    "X-Title": settings.app_name
                }
            )
            
            if stream:
                return self._stream_response(response)
            else:
                return self._parse_response(response)
                
        except Exception as e:
            logger.error("llm_error", error=str(e))
            raise
    
    async def _stream_response(self, response):
        """Stream chunks"""
        async for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
```

**ValidaÃ§Ã£o**:
- âœ… Consegue fazer chat simples
- âœ… Streaming funciona
- âœ… Retry em rate limit
- âœ… Logs estruturados

---

#### ğŸ” Task 2.2: MCP Integration (P0) - 2 dias
**Objetivo**: Busca de contexto funcionando

**Deliverables**:
- MCP client (subprocess communication)
- MÃ©todos: index, search, clear, status
- Error handling para MCP offline
- Caching de resultados
- Ranking de relevÃ¢ncia

**Code Example**:
```python
# backend/services/mcp.py

import asyncio
import json
from typing import List, Dict

class MCPClient:
    def __init__(self):
        self.process = None
        self.request_id = 0
    
    async def start(self):
        """Start MCP server process"""
        self.process = await asyncio.create_subprocess_exec(
            "npx", "-y", "@zilliz/claude-context-mcp@latest",
            stdin=asyncio.subprocess.PIPE,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            env={
                "OPENAI_API_KEY": settings.openai_api_key,
                "MILVUS_ADDRESS": settings.milvus_address,
                "MILVUS_TOKEN": settings.milvus_token
            }
        )
        
        logger.info("mcp_started", pid=self.process.pid)
    
    async def search_code(
        self,
        repository: str,
        query: str,
        limit: int = 10
    ) -> List[Dict]:
        """Search code in repository"""
        
        self.request_id += 1
        
        request = {
            "jsonrpc": "2.0",
            "id": self.request_id,
            "method": "tools/call",
            "params": {
                "name": "search_code",
                "arguments": {
                    "path": repository,
                    "query": query,
                    "limit": limit
                }
            }
        }
        
        # Send request
        self.process.stdin.write(
            json.dumps(request).encode() + b'\n'
        )
        await self.process.stdin.drain()
        
        # Read response
        response_line = await self.process.stdout.readline()
        response = json.loads(response_line.decode())
        
        if "error" in response:
            raise MCPError(response["error"])
        
        results = response.get("result", [])
        
        logger.info(
            "mcp_search_completed",
            query=query,
            results_count=len(results)
        )
        
        return results
```

**ValidaÃ§Ã£o**:
- âœ… Consegue buscar cÃ³digo
- âœ… Resultados relevantes (score > 0.7)
- âœ… Fallback se MCP offline
- âœ… LatÃªncia < 3s

---

#### ğŸ’¬ Task 2.3: Chat Flow (P0) - 2 dias
**Objetivo**: Conversa bÃ¡sica funcionando

**Deliverables**:
- Endpoint de mensagens
- HistÃ³rico de conversa
- Enriquecimento com MCP
- GeraÃ§Ã£o de respostas
- AtualizaÃ§Ã£o de spec progressivamente

**Flow**:
```
User sends message
  â†“
Backend receives
  â†“
MCP search (contexto)
  â†“
Build prompt (system + context + history + message)
  â†“
LLM generate response
  â†“
Update spec document
  â†“
Return response + spec_updates
  â†“
Frontend updates UI
```

**API**:
```
POST /api/conversations/message
{
  "session_id": "sess_xxx",
  "message": "Quero cache Redis"
}

Response:
{
  "message_id": "msg_yyy",
  "assistant_reply": "Entendi! Vou ajudar...",
  "context_used": [
    {
      "file": "backend/cache.py",
      "score": 0.92
    }
  ],
  "spec_updates": {
    "description": "Implementar cache Redis...",
    "technical_details": ["Item 1", "Item 2"]
  }
}
```

**ValidaÃ§Ã£o**:
- âœ… Conversa multi-turno funciona
- âœ… Contexto Ã© usado nas respostas
- âœ… Spec Ã© atualizada progressivamente
- âœ… HistÃ³rico Ã© mantido

---

### Sprint 3: Spec Generation (Dias 11-15)

#### ğŸ“ Task 3.1: Spec Generator (P0) - 2 dias
**Objetivo**: Gerar documento no formato da empresa

**Deliverables**:
- Template engine (usar company-task-template.md)
- Preenchimento progressivo
- ValidaÃ§Ã£o de completude
- FormataÃ§Ã£o Markdown

**Code Example**:
```python
# backend/services/spec_generator.py

class SpecGenerator:
    def __init__(self):
        with open("company-task-template.md") as f:
            self.template = f.read()
    
    def generate_spec(self, spec_data: SpecDocument) -> str:
        """Generate markdown from spec data"""
        
        sections = {
            "description": spec_data.description,
            "user_story": spec_data.user_story,
            "expected_results": self._format_list(spec_data.expected_results),
            "technical_details": self._format_list(spec_data.technical_details),
            "checklist": self._format_checklist(spec_data.checklist),
            "acceptance_criteria": self._format_list(spec_data.acceptance_criteria),
            "definition_of_done": self._format_list(spec_data.definition_of_done),
            "observations": spec_data.observations or "",
            "references": self._format_list(spec_data.references) if spec_data.references else "",
            "risks": self._format_list(spec_data.risks) if spec_data.risks else ""
        }
        
        # Replace template variables
        result = self.template
        for key, value in sections.items():
            result = result.replace(f"{{{key}}}", value)
        
        return result
    
    def validate_completeness(self, spec_data: SpecDocument) -> Dict:
        """Check how complete the spec is"""
        
        required_fields = [
            "description",
            "user_story",
            "expected_results",
            "technical_details",
            "acceptance_criteria"
        ]
        
        completed = sum(
            1 for field in required_fields
            if getattr(spec_data, field)
        )
        
        return {
            "total_fields": len(required_fields),
            "completed_fields": completed,
            "percentage": (completed / len(required_fields)) * 100,
            "missing_fields": [
                f for f in required_fields
                if not getattr(spec_data, f)
            ]
        }
```

**ValidaÃ§Ã£o**:
- âœ… Markdown gerado Ã© vÃ¡lido
- âœ… Todas seÃ§Ãµes presentes
- âœ… FormataÃ§Ã£o correta
- âœ… Emojis renderizam

---

#### ğŸ¯ Task 3.2: Profile Adaptation (P0) - 1 dia
**Objetivo**: IA adapta linguagem ao perfil

**Deliverables**:
- System prompts por perfil
- ValidaÃ§Ã£o de output por perfil
- Testes com ambos perfis

**Implementation**:
```python
# backend/services/conversation.py

class ConversationService:
    def __init__(self, llm: LLMService, mcp: MCPClient):
        self.llm = llm
        self.mcp = mcp
        self.prompts = PromptLibrary()
    
    async def process_message(
        self,
        session: Session,
        message: str
    ) -> Dict:
        """Process user message"""
        
        # Get context from MCP
        context = await self.mcp.search_code(
            repository=session.selected_repositories[0].path,
            query=message,
            limit=10
        )
        
        # Build messages for LLM
        messages = [
            {
                "role": "system",
                "content": self.prompts.system_prompt(session.user_profile)
            },
            *self._format_history(session.conversation_history),
            {
                "role": "user",
                "content": self.prompts.user_message_prompt(
                    message=message,
                    context=context,
                    profile=session.user_profile
                )
            }
        ]
        
        # Get LLM response
        response = await self.llm.chat(messages)
        
        # Parse and extract spec updates
        spec_updates = self._extract_spec_updates(response)
        
        return {
            "assistant_reply": response["content"],
            "context_used": context,
            "spec_updates": spec_updates
        }
```

**ValidaÃ§Ã£o**:
- âœ… NÃ£o-tÃ©cnico recebe linguagem simples
- âœ… TÃ©cnico recebe detalhes tÃ©cnicos
- âœ… Tradeoffs adaptados ao perfil

---

#### ğŸ“¥ Task 3.3: Export (P0) - 1 dia
**Objetivo**: Download de markdown

**Deliverables**:
- Endpoint de export
- Download de arquivo .md
- Copy to clipboard (frontend)
- Formatos: MD, JSON, YAML

**API**:
```
POST /api/export
{
  "session_id": "sess_xxx",
  "format": "markdown"
}

Response:
{
  "specs": {
    "backend-api": "# ğŸ“Œ DescriÃ§Ã£o..."
  }
}
```

**Frontend**:
```typescript
// Download MD
const downloadMd = async () => {
  const response = await api.export(sessionId, 'markdown');
  const blob = new Blob([response.specs['backend-api']], 
    { type: 'text/markdown' });
  const url = URL.createObjectURL(blob);
  const a = document.createElement('a');
  a.href = url;
  a.download = 'spec.md';
  a.click();
};

// Copy to clipboard
const copyToClipboard = async () => {
  const response = await api.export(sessionId, 'markdown');
  await navigator.clipboard.writeText(response.specs['backend-api']);
  toast.success('Copiado!');
};
```

**ValidaÃ§Ã£o**:
- âœ… Download funciona
- âœ… Copy funciona
- âœ… Formatos diferentes OK

---

## ğŸ¨ FASE 2: Polish & Testing (Semana 4)

### Sprint 4: UX & Features (Dias 16-20)

#### ğŸ¤ Task 4.1: Voice Input (P1) - 2 dias
**Objetivo**: UsuÃ¡rio pode falar ao invÃ©s de digitar

**Deliverables**:
- Web Speech API integration
- TranscriÃ§Ã£o em tempo real
- UI de gravaÃ§Ã£o
- Fallback para Whisper API (opcional)

**Frontend**:
```typescript
// components/VoiceInput.tsx

import { useState, useRef } from 'react';

const VoiceInput = ({ onTranscript }) => {
  const [isRecording, setIsRecording] = useState(false);
  const [transcript, setTranscript] = useState('');
  const recognitionRef = useRef(null);
  
  const startRecording = () => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    
    if (!SpeechRecognition) {
      alert('Navegador nÃ£o suporta reconhecimento de voz');
      return;
    }
    
    const recognition = new SpeechRecognition();
    recognition.lang = 'pt-BR';
    recognition.continuous = true;
    recognition.interimResults = true;
    
    recognition.onresult = (event) => {
      const current = event.resultIndex;
      const transcript = event.results[current][0].transcript;
      setTranscript(transcript);
    };
    
    recognition.start();
    setIsRecording(true);
    recognitionRef.current = recognition;
  };
  
  const stopRecording = () => {
    recognitionRef.current?.stop();
    setIsRecording(false);
    onTranscript(transcript);
  };
  
  return (
    <div>
      {!isRecording ? (
        <Button onClick={startRecording}>
          ğŸ¤ Falar
        </Button>
      ) : (
        <div>
          <div className="recording-indicator">
            ğŸ”´ Gravando... {transcript}
          </div>
          <Button onClick={stopRecording}>
            â¹ï¸ Parar
          </Button>
        </div>
      )}
    </div>
  );
};
```

**ValidaÃ§Ã£o**:
- âœ… Grava e transcreve em portuguÃªs
- âœ… Mostra transcriÃ§Ã£o em tempo real
- âœ… UI clara e intuitiva
- âœ… Fallback se nÃ£o suportado

---

#### ğŸ“Š Task 4.2: Preview Side-by-Side (P1) - 1 dia
**Objetivo**: Ver documento sendo construÃ­do

**Deliverables**:
- Split view (chat + preview)
- Markdown rendering (react-markdown)
- Auto-scroll to latest section
- Toggle entre modos (chat-only, split, preview-only)

**Frontend**:
```typescript
// components/PreviewPane.tsx

import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';

const PreviewPane = ({ markdown }) => {
  return (
    <div className="preview-pane">
      <div className="preview-header">
        <h3>ğŸ“„ Preview do Documento</h3>
        <div className="completeness-badge">
          90% completo
        </div>
      </div>
      
      <ReactMarkdown 
        remarkPlugins={[remarkGfm]}
        className="markdown-body"
      >
        {markdown}
      </ReactMarkdown>
    </div>
  );
};
```

**ValidaÃ§Ã£o**:
- âœ… Preview atualiza em tempo real
- âœ… Markdown renderiza corretamente
- âœ… Scroll automÃ¡tico funciona
- âœ… Toggle entre modos OK

---

#### âš ï¸ Task 4.3: Tech Debt Detector (P1) - 2 dias
**Objetivo**: AnÃ¡lise inteligente de dÃ­vida tÃ©cnica

**Deliverables**:
- Endpoint de anÃ¡lise
- Prompt especializado
- ClassificaÃ§Ã£o por severidade
- UI de resultados

**Backend**:
```python
# backend/api/analysis.py

@app.post("/api/analysis/tech-debt")
async def analyze_tech_debt(
    request: AnalyzeTechDebtRequest,
    background_tasks: BackgroundTasks
):
    """Analyze tech debt using AI"""
    
    session = sessions.get(request.session_id)
    if not session:
        raise HTTPException(404, "Session not found")
    
    # Get code context
    code_context = []
    for repo in session.selected_repositories:
        results = await mcp_client.search_code(
            repository=repo.path,
            query=f"{session.specs[repo.name].title} {session.specs[repo.name].description}",
            limit=15
        )
        code_context.extend(results)
    
    # Build specialized prompt
    prompt = prompts.tech_debt_analysis_prompt(
        feature_title=session.specs[repo.name].title,
        code_context=code_context,
        tech_stack=session.tech_stack
    )
    
    # Call LLM
    response = await llm_service.chat([
        {"role": "system", "content": "You are a senior software architect"},
        {"role": "user", "content": prompt}
    ])
    
    # Parse JSON response
    analysis = json.loads(response["content"])
    
    return AnalyzeTechDebtResponse(
        total_items=analysis["summary"]["total_issues"],
        critical=analysis["tech_debt"][:3],  # Top 3 critical
        medium=analysis["tech_debt"][3:6],
        low=analysis["tech_debt"][6:],
        total_effort_hours=analysis["summary"]["total_effort_hours"]
    )
```

**Frontend UI**:
```typescript
// components/TechDebtReport.tsx

const TechDebtReport = ({ analysis }) => {
  return (
    <div className="tech-debt-report">
      <div className="summary">
        ğŸ“Š {analysis.total_items} problemas | 
        {analysis.total_effort_hours}h esforÃ§o total
      </div>
      
      {analysis.critical.length > 0 && (
        <div className="section critical">
          <h3>ğŸ”´ CrÃ­ticos ({analysis.critical.length})</h3>
          {analysis.critical.map(item => (
            <TechDebtItem key={item.file} item={item} />
          ))}
        </div>
      )}
      
      {/* Similar for medium and low */}
      
      <div className="actions">
        <Button onClick={addAllToSpec}>
          âœ… Adicionar Todos Ã  Spec
        </Button>
        <Button variant="outline" onClick={selectItems}>
          âš™ï¸ Selecionar
        </Button>
      </div>
    </div>
  );
};
```

**ValidaÃ§Ã£o**:
- âœ… AnÃ¡lise retorna problemas reais
- âœ… ClassificaÃ§Ã£o por severidade correta
- âœ… SugestÃµes sÃ£o acionÃ¡veis
- âœ… UI clara e navegÃ¡vel

---

## ğŸš€ FASE 3: Advanced Features (Semanas 5-6)

### Sprint 5: Multi-Spec & Review (Dias 21-25)

#### ğŸ”¢ Task 5.1: Multi-Spec Detection (P1) - 3 dias
**Objetivo**: Detectar quando precisa mÃºltiplas specs

**Deliverables**:
- AnÃ¡lise de impacto multi-repo
- DetecÃ§Ã£o automÃ¡tica (nÃ£o-tÃ©cnico)
- DetecÃ§Ã£o manual (tÃ©cnico)
- GeraÃ§Ã£o de specs linkadas
- Export coordenado

**ValidaÃ§Ã£o**:
- âœ… Detecta corretamente (>90% accuracy)
- âœ… Gera specs separadas
- âœ… Linking entre specs funciona
- âœ… Export cria arquivos linkados

---

#### âœ… Task 5.2: Review Mode (P2) - 2 dias
**Objetivo**: Workflow de aprovaÃ§Ã£o

**Deliverables**:
- Estados: Draft â†’ Review â†’ Approved
- @mentions
- ComentÃ¡rios inline
- PÃ¡gina separada de review

**ValidaÃ§Ã£o**:
- âœ… TransiÃ§Ãµes de estado funcionam
- âœ… @mentions notificam
- âœ… ComentÃ¡rios salvos
- âœ… AprovaÃ§Ã£o registrada

---

### Sprint 6: Final Polish (Dias 26-30)

#### ğŸ”’ Task 6.1: Security Checklist (P1) - 1 dia
#### ğŸ“Š Task 6.2: Mermaid Diagrams (P2) - 1 dia
#### ğŸ“ Task 6.3: Interactive Tutorial (P1) - 1 dia
#### ğŸ§ª Task 6.4: E2E Testing (P0) - 2 dias

---

## ğŸ“‹ Definition of Done para MVP

### Core Functionality
- [x] Docker compose up funciona
- [ ] Chat multi-turno OK
- [ ] MCP busca contexto relevante
- [ ] LLM gera respostas adaptadas ao perfil
- [ ] Spec Ã© gerada no formato da empresa
- [ ] Export para markdown funciona

### Advanced Features
- [ ] Voice input funciona
- [ ] Preview side-by-side OK
- [ ] Tech debt analysis retorna resultados Ãºteis
- [ ] Multi-spec detecta corretamente

### Quality
- [ ] > 80% test coverage
- [ ] E2E tests passam
- [ ] Performance targets OK (< 3s MCP, < 5s LLM)
- [ ] Sem erros crÃ­ticos

### Documentation
- [ ] README com setup instructions
- [ ] API docs (auto-generated)
- [ ] User guide bÃ¡sico

---

**PrÃ³ximo Passo**: Executar `/tasks` para gerar tasks implementÃ¡veis

